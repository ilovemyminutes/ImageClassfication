{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "sys.path.insert(0, '../')\n",
    "from utils import load_json, save_pickle\n",
    "from model import VanillaEfficientNet, load_model\n",
    "from dataset import TrainDataset, EvalDataset\n",
    "from torchvision import transforms\n",
    "from transform_settings import configure_transform\n",
    "from albumentations import CenterCrop, Blur, Cutout, Equalize, GaussianBlur, GaussNoise, GlassBlur, GridDistortion, Lambda, MedianBlur, MotionBlur, Normalize, RandomBrightnessContrast, RandomFog, Solarize\n",
    "from albumentations.pytorch.transforms import ToTensor"
   ]
  },
  {
   "source": [
    "# Pairing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(batch, alpha):\n",
    "    data, targets = batch\n",
    "\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    image_h, image_w = data.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "class CutMixCollator:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
    "        batch = cutmix(batch, self.alpha)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class CutMixCriterion:\n",
    "    def __init__(self, reduction):\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        targets1, targets2, lam = targets\n",
    "        return lam * self.criterion(\n",
    "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 384), Image.BILINEAR),\n",
    "    transforms.CenterCrop((384, 384)),\n",
    "    # transforms.RandomResizedCrop((224, 224)),\n",
    "    ])\n",
    "\n",
    "data_config = {\n",
    "    'root': '../preprocessed_stratified/train', \n",
    "    'transform': transform, \n",
    "    'task': 'main',\n",
    "    'meta_path': '../preprocessed_stratified/metadata.json'\n",
    "    }\n",
    "dataset = TrainDataset(**data_config)\n",
    "model = VanillaEfficientNet(n_class=18)\n",
    "loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ageg_gender(x):\n",
    "    name = os.path.basename(x)\n",
    "    gender = name.split('_')[1]\n",
    "    try:\n",
    "        age = int(name.split('_')[-2])\n",
    "    except:\n",
    "        age = int(name.split('_')[-3])\n",
    "\n",
    "    CLASSES = {('young', 'female'): 0, ('young', 'male'): 1, ('middle', 'female'): 2, ('middle', 'male'):3, ('old', 'female'): 4, ('old', 'male'): 5}\n",
    "    if age < 30:\n",
    "        age = 'young'\n",
    "    elif age >= 30 and age < 60:\n",
    "        age = 'middle'\n",
    "    else:\n",
    "        age = 'old'\n",
    "    \n",
    "    if gender == 'female':\n",
    "        gender = 'female'\n",
    "    else:\n",
    "        gender = 'male'\n",
    "\n",
    "    return CLASSES[(age, gender)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2    4298\n",
       "0    3850\n",
       "1    2891\n",
       "3    2156\n",
       "4     581\n",
       "5     441\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "img_paths = glob('../preprocessed_stratified/train/*')\n",
    "metadata  = metadata = load_json('../preprocessed/metadata.json')\n",
    "train_imgs = pd.DataFrame(dict(path=img_paths))\n",
    "train_imgs['class'] = train_imgs['path'].apply(lambda x: get_class_ageg_gender(x))\n",
    "train_imgs['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NUM = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentation_nums = 5000 - train_imgs['class'].value_counts()\n",
    "augmentation_nums = {i:j for i, j in zip(augmentation_nums.index.tolist(), augmentation_nums.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../input/data/train/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 0\n",
    "N = augmentation_nums[CLASS]\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(N):\n",
    "    sample = np.random.choice(train_imgs[train_imgs['class'] == CLASS]['path'].tolist(), size=2, replace=True).tolist()\n",
    "    s1 = np.array(Image.open(sample[0]))\n",
    "    s2 = np.array(Image.open(sample[1]))\n",
    "\n",
    "    s1_half = s1[:, :192, :]\n",
    "    s2_half = s2[:, 192:, :]\n",
    "\n",
    "    paired = np.hstack([s1_half, s2_half])\n",
    "    break\n"
   ]
  },
  {
   "source": [
    "# Face Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-f52b97ed43f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install cmake Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlib_to_cv_bounding_box(box):\n",
    "    # convert dlib bounding box for OpenCV display\n",
    "    x = box.left()\n",
    "    y = box.top()\n",
    "    w = box.right() - x\n",
    "    h = box.bottom() - y\n",
    "\n",
    "    return x, y, w, h\n",
    "\n",
    "def landmarks_to_numpy(landmarks):\n",
    "    # initialize the matrix of (x, y)-coordinates with a row for each landmark\n",
    "    coords = np.zeros((landmarks.num_parts, 2), dtype=int)\n",
    "    # convert each landmark to (x, y)\n",
    "    for i in range(0, landmarks.num_parts):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "    # return the array of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def detect_landmarks(img):\n",
    "    points = []\n",
    "    detected_faces = frontal_face_detector(img, 1)\n",
    "    for face in detected_faces:\n",
    "        landmarks = landmarks_predictor(img, face)\n",
    "        points.append(landmarks_to_numpy(landmarks))\n",
    " \n",
    "    return landmarks, points, detected_faces, img"
   ]
  }
 ]
}