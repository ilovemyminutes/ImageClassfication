{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from dataset import TrainDataset\n",
    "from transform_settings import configure_transform\n",
    "from utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../preprocessed/train'\n",
    "TASK = 'all'\n",
    "META = '../preprocessed/metadata.json'\n",
    "transform = configure_transform('train', 'base')\n",
    "data = TrainDataset(ROOT, transform, TASK, META)\n",
    "loader = DataLoader(data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, labels in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class THAResNet(nn.Module): # Three-headed attention EfficientNEt\n",
    "    def __init__(self):\n",
    "        super(THAResNet, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.linear_mask = nn.Linear(1000, 3)\n",
    "        self.linear_ageg = nn.Linear(1000, 3)\n",
    "        self.linear_gender = nn.Linear(1000, 2)\n",
    "\n",
    "        self._freeze()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        output_mask = self.linear_mask(x)\n",
    "        output_ageg = self.linear_ageg(x)\n",
    "        output_gender = self.linear_gender(x)\n",
    "        return output_mask, output_ageg, output_gender\n",
    "\n",
    "    def _freeze(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = THAResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_mask = optim.Adam(model.linear_mask.parameters(), lr=.005)\n",
    "optim_ageg = optim.Adam(model.linear_ageg.parameters(), lr=.005)\n",
    "optim_gender = optim.Adam(model.linear_gender.parameters(), lr=.005)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mask, output_ageg, output_gender = model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask = criterion(output_mask, labels['mask'])\n",
    "loss_ageg = criterion(output_ageg, labels['ageg'])\n",
    "loss_gender = criterion(output_gender, labels['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.5958, grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "loss_mask /= 2\n",
    "loss_ageg /= 2\n",
    "loss_gender /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "post_resnet = resnet(img)\n",
    "post_resnet.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_CLASSES = 3\n",
    "SEX_CLASSES = 2\n",
    "AGEG_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def get_linear_dropout(in_features, out_features, p=.5):\n",
    "    layer = nn.Sequential(OrderedDict([\n",
    "        ('linear', nn.Linear(in_features=in_features, out_features=out_features)), \n",
    "        ('dropout', nn.Dropout(p=0.5))]))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_mask = get_linear_dropout(in_features=2048, out_features=3)\n",
    "fc_sex = get_linear_dropout(in_features=2048, out_features=2)\n",
    "fc_ageg = get_linear_dropout(in_features=2048, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer = nn.ModuleList([fc_mask, fc_sex, fc_ageg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for fc in fc_layer:\n",
    "    outputs.append(fc(post_resnet))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.LongTensor([0 for _ in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.9253, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "F.cross_entropy(outputs[0], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ModuleList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.weight\nbn1.weight\nbn1.bias\nlayer1.0.conv1.weight\nlayer1.0.bn1.weight\nlayer1.0.bn1.bias\nlayer1.0.conv2.weight\nlayer1.0.bn2.weight\nlayer1.0.bn2.bias\nlayer1.0.conv3.weight\nlayer1.0.bn3.weight\nlayer1.0.bn3.bias\nlayer1.0.downsample.0.weight\nlayer1.0.downsample.1.weight\nlayer1.0.downsample.1.bias\nlayer1.1.conv1.weight\nlayer1.1.bn1.weight\nlayer1.1.bn1.bias\nlayer1.1.conv2.weight\nlayer1.1.bn2.weight\nlayer1.1.bn2.bias\nlayer1.1.conv3.weight\nlayer1.1.bn3.weight\nlayer1.1.bn3.bias\nlayer1.2.conv1.weight\nlayer1.2.bn1.weight\nlayer1.2.bn1.bias\nlayer1.2.conv2.weight\nlayer1.2.bn2.weight\nlayer1.2.bn2.bias\nlayer1.2.conv3.weight\nlayer1.2.bn3.weight\nlayer1.2.bn3.bias\nlayer2.0.conv1.weight\nlayer2.0.bn1.weight\nlayer2.0.bn1.bias\nlayer2.0.conv2.weight\nlayer2.0.bn2.weight\nlayer2.0.bn2.bias\nlayer2.0.conv3.weight\nlayer2.0.bn3.weight\nlayer2.0.bn3.bias\nlayer2.0.downsample.0.weight\nlayer2.0.downsample.1.weight\nlayer2.0.downsample.1.bias\nlayer2.1.conv1.weight\nlayer2.1.bn1.weight\nlayer2.1.bn1.bias\nlayer2.1.conv2.weight\nlayer2.1.bn2.weight\nlayer2.1.bn2.bias\nlayer2.1.conv3.weight\nlayer2.1.bn3.weight\nlayer2.1.bn3.bias\nlayer2.2.conv1.weight\nlayer2.2.bn1.weight\nlayer2.2.bn1.bias\nlayer2.2.conv2.weight\nlayer2.2.bn2.weight\nlayer2.2.bn2.bias\nlayer2.2.conv3.weight\nlayer2.2.bn3.weight\nlayer2.2.bn3.bias\nlayer2.3.conv1.weight\nlayer2.3.bn1.weight\nlayer2.3.bn1.bias\nlayer2.3.conv2.weight\nlayer2.3.bn2.weight\nlayer2.3.bn2.bias\nlayer2.3.conv3.weight\nlayer2.3.bn3.weight\nlayer2.3.bn3.bias\nlayer3.0.conv1.weight\nlayer3.0.bn1.weight\nlayer3.0.bn1.bias\nlayer3.0.conv2.weight\nlayer3.0.bn2.weight\nlayer3.0.bn2.bias\nlayer3.0.conv3.weight\nlayer3.0.bn3.weight\nlayer3.0.bn3.bias\nlayer3.0.downsample.0.weight\nlayer3.0.downsample.1.weight\nlayer3.0.downsample.1.bias\nlayer3.1.conv1.weight\nlayer3.1.bn1.weight\nlayer3.1.bn1.bias\nlayer3.1.conv2.weight\nlayer3.1.bn2.weight\nlayer3.1.bn2.bias\nlayer3.1.conv3.weight\nlayer3.1.bn3.weight\nlayer3.1.bn3.bias\nlayer3.2.conv1.weight\nlayer3.2.bn1.weight\nlayer3.2.bn1.bias\nlayer3.2.conv2.weight\nlayer3.2.bn2.weight\nlayer3.2.bn2.bias\nlayer3.2.conv3.weight\nlayer3.2.bn3.weight\nlayer3.2.bn3.bias\nlayer3.3.conv1.weight\nlayer3.3.bn1.weight\nlayer3.3.bn1.bias\nlayer3.3.conv2.weight\nlayer3.3.bn2.weight\nlayer3.3.bn2.bias\nlayer3.3.conv3.weight\nlayer3.3.bn3.weight\nlayer3.3.bn3.bias\nlayer3.4.conv1.weight\nlayer3.4.bn1.weight\nlayer3.4.bn1.bias\nlayer3.4.conv2.weight\nlayer3.4.bn2.weight\nlayer3.4.bn2.bias\nlayer3.4.conv3.weight\nlayer3.4.bn3.weight\nlayer3.4.bn3.bias\nlayer3.5.conv1.weight\nlayer3.5.bn1.weight\nlayer3.5.bn1.bias\nlayer3.5.conv2.weight\nlayer3.5.bn2.weight\nlayer3.5.bn2.bias\nlayer3.5.conv3.weight\nlayer3.5.bn3.weight\nlayer3.5.bn3.bias\nlayer3.6.conv1.weight\nlayer3.6.bn1.weight\nlayer3.6.bn1.bias\nlayer3.6.conv2.weight\nlayer3.6.bn2.weight\nlayer3.6.bn2.bias\nlayer3.6.conv3.weight\nlayer3.6.bn3.weight\nlayer3.6.bn3.bias\nlayer3.7.conv1.weight\nlayer3.7.bn1.weight\nlayer3.7.bn1.bias\nlayer3.7.conv2.weight\nlayer3.7.bn2.weight\nlayer3.7.bn2.bias\nlayer3.7.conv3.weight\nlayer3.7.bn3.weight\nlayer3.7.bn3.bias\nlayer3.8.conv1.weight\nlayer3.8.bn1.weight\nlayer3.8.bn1.bias\nlayer3.8.conv2.weight\nlayer3.8.bn2.weight\nlayer3.8.bn2.bias\nlayer3.8.conv3.weight\nlayer3.8.bn3.weight\nlayer3.8.bn3.bias\nlayer3.9.conv1.weight\nlayer3.9.bn1.weight\nlayer3.9.bn1.bias\nlayer3.9.conv2.weight\nlayer3.9.bn2.weight\nlayer3.9.bn2.bias\nlayer3.9.conv3.weight\nlayer3.9.bn3.weight\nlayer3.9.bn3.bias\nlayer3.10.conv1.weight\nlayer3.10.bn1.weight\nlayer3.10.bn1.bias\nlayer3.10.conv2.weight\nlayer3.10.bn2.weight\nlayer3.10.bn2.bias\nlayer3.10.conv3.weight\nlayer3.10.bn3.weight\nlayer3.10.bn3.bias\nlayer3.11.conv1.weight\nlayer3.11.bn1.weight\nlayer3.11.bn1.bias\nlayer3.11.conv2.weight\nlayer3.11.bn2.weight\nlayer3.11.bn2.bias\nlayer3.11.conv3.weight\nlayer3.11.bn3.weight\nlayer3.11.bn3.bias\nlayer3.12.conv1.weight\nlayer3.12.bn1.weight\nlayer3.12.bn1.bias\nlayer3.12.conv2.weight\nlayer3.12.bn2.weight\nlayer3.12.bn2.bias\nlayer3.12.conv3.weight\nlayer3.12.bn3.weight\nlayer3.12.bn3.bias\nlayer3.13.conv1.weight\nlayer3.13.bn1.weight\nlayer3.13.bn1.bias\nlayer3.13.conv2.weight\nlayer3.13.bn2.weight\nlayer3.13.bn2.bias\nlayer3.13.conv3.weight\nlayer3.13.bn3.weight\nlayer3.13.bn3.bias\nlayer3.14.conv1.weight\nlayer3.14.bn1.weight\nlayer3.14.bn1.bias\nlayer3.14.conv2.weight\nlayer3.14.bn2.weight\nlayer3.14.bn2.bias\nlayer3.14.conv3.weight\nlayer3.14.bn3.weight\nlayer3.14.bn3.bias\nlayer3.15.conv1.weight\nlayer3.15.bn1.weight\nlayer3.15.bn1.bias\nlayer3.15.conv2.weight\nlayer3.15.bn2.weight\nlayer3.15.bn2.bias\nlayer3.15.conv3.weight\nlayer3.15.bn3.weight\nlayer3.15.bn3.bias\nlayer3.16.conv1.weight\nlayer3.16.bn1.weight\nlayer3.16.bn1.bias\nlayer3.16.conv2.weight\nlayer3.16.bn2.weight\nlayer3.16.bn2.bias\nlayer3.16.conv3.weight\nlayer3.16.bn3.weight\nlayer3.16.bn3.bias\nlayer3.17.conv1.weight\nlayer3.17.bn1.weight\nlayer3.17.bn1.bias\nlayer3.17.conv2.weight\nlayer3.17.bn2.weight\nlayer3.17.bn2.bias\nlayer3.17.conv3.weight\nlayer3.17.bn3.weight\nlayer3.17.bn3.bias\nlayer3.18.conv1.weight\nlayer3.18.bn1.weight\nlayer3.18.bn1.bias\nlayer3.18.conv2.weight\nlayer3.18.bn2.weight\nlayer3.18.bn2.bias\nlayer3.18.conv3.weight\nlayer3.18.bn3.weight\nlayer3.18.bn3.bias\nlayer3.19.conv1.weight\nlayer3.19.bn1.weight\nlayer3.19.bn1.bias\nlayer3.19.conv2.weight\nlayer3.19.bn2.weight\nlayer3.19.bn2.bias\nlayer3.19.conv3.weight\nlayer3.19.bn3.weight\nlayer3.19.bn3.bias\nlayer3.20.conv1.weight\nlayer3.20.bn1.weight\nlayer3.20.bn1.bias\nlayer3.20.conv2.weight\nlayer3.20.bn2.weight\nlayer3.20.bn2.bias\nlayer3.20.conv3.weight\nlayer3.20.bn3.weight\nlayer3.20.bn3.bias\nlayer3.21.conv1.weight\nlayer3.21.bn1.weight\nlayer3.21.bn1.bias\nlayer3.21.conv2.weight\nlayer3.21.bn2.weight\nlayer3.21.bn2.bias\nlayer3.21.conv3.weight\nlayer3.21.bn3.weight\nlayer3.21.bn3.bias\nlayer3.22.conv1.weight\nlayer3.22.bn1.weight\nlayer3.22.bn1.bias\nlayer3.22.conv2.weight\nlayer3.22.bn2.weight\nlayer3.22.bn2.bias\nlayer3.22.conv3.weight\nlayer3.22.bn3.weight\nlayer3.22.bn3.bias\nlayer4.0.conv1.weight\nlayer4.0.bn1.weight\nlayer4.0.bn1.bias\nlayer4.0.conv2.weight\nlayer4.0.bn2.weight\nlayer4.0.bn2.bias\nlayer4.0.conv3.weight\nlayer4.0.bn3.weight\nlayer4.0.bn3.bias\nlayer4.0.downsample.0.weight\nlayer4.0.downsample.1.weight\nlayer4.0.downsample.1.bias\nlayer4.1.conv1.weight\nlayer4.1.bn1.weight\nlayer4.1.bn1.bias\nlayer4.1.conv2.weight\nlayer4.1.bn2.weight\nlayer4.1.bn2.bias\nlayer4.1.conv3.weight\nlayer4.1.bn3.weight\nlayer4.1.bn3.bias\nlayer4.2.conv1.weight\nlayer4.2.bn1.weight\nlayer4.2.bn1.bias\nlayer4.2.conv2.weight\nlayer4.2.bn2.weight\nlayer4.2.bn2.bias\nlayer4.2.conv3.weight\nlayer4.2.bn3.weight\nlayer4.2.bn3.bias\nfc.weight\nfc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in resnet.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}