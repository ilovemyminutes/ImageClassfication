{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "sys.path.insert(0, '../')\n",
    "from utils import load_json, save_pickle\n",
    "from model import VanillaEfficientNet, load_model\n",
    "from dataset import TrainDataset, EvalDataset\n",
    "from torchvision import transforms\n",
    "from transform_settings import configure_transform\n",
    "from albumentations import CenterCrop, Blur, Cutout, Equalize, GaussianBlur, GaussNoise, GlassBlur, GridDistortion, Lambda, MedianBlur, MotionBlur, Normalize, RandomBrightnessContrast, RandomFog, Solarize, Resize, ToGray\n",
    "from albumentations.pytorch.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 384), Image.BILINEAR),\n",
    "    transforms.CenterCrop((384, 384)),\n",
    "    # transforms.RandomResizedCrop((224, 224)),\n",
    "    ])\n",
    "\n",
    "data_config = {\n",
    "    'root': '../preprocessed_stratified/train', \n",
    "    'transform': None, \n",
    "    'task': 'main',\n",
    "    'meta_path': '../preprocessed_stratified/metadata.json'\n",
    "    }\n",
    "dataset = TrainDataset(**data_config)\n",
    "model = VanillaEfficientNet(n_class=18)\n",
    "loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(pixel_dist_left, pixel_dist_right):\n",
    "    \"\"\"\n",
    "    픽셀 분포 간 유사도를 측정하는 함수\n",
    "    분포 간 intersection을 유사도로 가정하여 높을 수록 유사도가 높다\n",
    "    \"\"\"\n",
    "    hist_left = cv2.calcHist(images=[pixel_dist_left], channels=[0], mask=None, histSize=[32], ranges=[0,256])\n",
    "    hist_right = cv2.calcHist(images=[pixel_dist_right], channels=[0], mask=None, histSize=[32], ranges=[0,256])\n",
    "    similarity = cv2.compareHist(H1=hist_left, H2=hist_right, method=cv2.HISTCMP_INTERSECT)\n",
    "    return similarity\n",
    "\n",
    "def split_image(img: np.array, v, scale):\n",
    "    \"\"\"수직선을 기준으로 이미지를 분할\n",
    "    좌우로 이미지를 분할했을 때, 가로 길이가 같도록 가로 길이를 조정\n",
    "    \n",
    "    Args\n",
    "    ---\n",
    "    img: np.array, grayscale\n",
    "    v: 수직선의 위치\n",
    "    scale: crop 과정에 활용된 scale\n",
    "    \"\"\"\n",
    "    if v >= scale:\n",
    "        margin = scale - v%scale\n",
    "        pixel_dist_left = img[:, v-margin:v]\n",
    "        pixel_dist_right = img[:, v:]\n",
    "    else:\n",
    "        margin = v%scale\n",
    "        pixel_dist_left = img[:, :v]\n",
    "        pixel_dist_right = img[:, v:v+margin]\n",
    "    return pixel_dist_left, pixel_dist_right\n",
    "\n",
    "\n",
    "def get_vline(img_resized: np.array, scale: int=64):\n",
    "    v_list = [i+int(scale*(2/5)) for i in range(scale//5)][::2]\n",
    "    margin_list = [i+int(scale*(2/5)) for i in range(scale//5)][::2]\n",
    "    importances = np.zeros(len(v_list))\n",
    "\n",
    "    for idx, v in enumerate(v_list):\n",
    "        pixel_dist_left, pixel_dist_right = split_image(img_resized, v, scale)\n",
    "        similarity = get_similarity(pixel_dist_left, pixel_dist_right)\n",
    "        importances[idx] += (similarity / (v*scale)) # normalize\n",
    "    v = v_list[np.argsort(importances)[3]] # (경험적) importance가 가장 높은 값이 아닌 2~3번째로 높은 값이 원하는 수직선이 나오는 경우가 많음\n",
    "    return v\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "\n",
    "def get_hline(img: np.array, v):\n",
    "    UPPER_QUANTILE = 0.35\n",
    "    LOWER_QUANTILE = 1 - UPPER_QUANTILE\n",
    "    FACTOR = 8\n",
    "\n",
    "    v0 = moving_average(img_resized[:, v], FACTOR) # 단순이동평균\n",
    "    v1 = np.roll(v0, shift=1, axis=0)\n",
    "    sma_interval = v0.shape[0]\n",
    "    offset = int(sma_interval * LOWER_QUANTILE)\n",
    "\n",
    "    h_upper = np.argmax(np.abs(v0[1:int(sma_interval*UPPER_QUANTILE)] - v1[1:int(sma_interval*UPPER_QUANTILE)])) + FACTOR - 1\n",
    "    h_lower = np.argmax(np.abs(v0[int(sma_interval*LOWER_QUANTILE):-1] - v1[int(sma_interval*LOWER_QUANTILE):-1])) + offset - FACTOR + 1\n",
    "\n",
    "    return h_upper, h_lower\n",
    "\n",
    "\n",
    "def crop(img, scale: int=64, p=0.5, rescaled=True):\n",
    "    if np.random.uniform(0, 1) >= p: # 시행하지 않을 경우\n",
    "        return img\n",
    "        \n",
    "    transform = transforms.Compose([transforms.Resize((int(scale*(4/3)), scale)), transforms.Grayscale()])\n",
    "    # transform = A.Compose([Resize((int(scale*(4/3)), scale)), ToGray])\n",
    "    img_resized = np.array(transform(img))\n",
    "    MAX_H, MAX_W = np.array(img).shape[:2]\n",
    "\n",
    "    vline = get_vline(img_resized, scale)\n",
    "    h_upper, h_lower = get_hline(img_resized, vline)\n",
    "\n",
    "    if rescaled: # 입력할 때의 이미지 크기에 맞게 수직선의 위치를 조정\n",
    "        origin_v_scale = np.array(img).shape[1]\n",
    "        origin_h_scale = np.array(img).shape[0]\n",
    "\n",
    "        vline = int(origin_v_scale * (vline / scale))\n",
    "        h_lower = int(origin_h_scale * (h_lower / int(scale*(4/3))))\n",
    "        h_upper = int(origin_h_scale * (h_upper / int(scale*(4/3))))\n",
    "\n",
    "    hline = int((h_lower + h_upper) / 2)\n",
    "    height = abs(h_upper - h_lower)\n",
    "    height = int(1.1 * height)\n",
    "    width = int(height * (3/4))\n",
    "\n",
    "    coord_y = np.clip([hline-height, hline+height], 0, MAX_H)\n",
    "    coord_x = np.clip([vline-width, vline+width], 0, MAX_W)\n",
    "    cropped = np.array(img)[coord_y[0]:coord_y[1], coord_x[0]:coord_x[1], :]\n",
    "\n",
    "    return Image.fromarray(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "357.3333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 743
    }
   ],
   "source": [
    "268 * (4/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "312.0"
      ]
     },
     "metadata": {},
     "execution_count": 751
    }
   ],
   "source": [
    "234 * (4/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.88 ms ± 148 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: crop(x)),\n",
    "    transforms.Resize((312, 234)),\n",
    "    transforms.RandomCrop((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "def foo():\n",
    "    transform(dataset[20][0])\n",
    "%timeit foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'image'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-713-c4c2bc11657e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'image'"
     ]
    }
   ],
   "source": [
    "transform(image=np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'cols'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-577-e37f032eef6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     )\n\u001b[1;32m     88\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m   3075\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_apply_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3077\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_to_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'cols'"
     ]
    }
   ],
   "source": [
    "aug(image=np.array(sample))"
   ]
  }
 ]
}